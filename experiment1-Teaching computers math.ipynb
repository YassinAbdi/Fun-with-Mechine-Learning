{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abstract:\n",
    "    Over the last couple of decades mechine learning as became a techinology that has proven itself a useful tools for science. For example scientist at National Aeronotics and Space Agency(NASA) have used machine learning to detect exoplants form the data retrived  Kepler sattilite. This and other example of remarkable achivements by industry makes mechine learning one of the most studied topics of our time. In this expiment the hope is to build a mechine learning aligrothium that learns how to do basic math. Starting with addition and ending with diviation the goal is to write a simple mathatical statement like 1+1 and expect the algrothium to return 2 with high confidence.My hypothsis is that with every addition of a class of operation the algortiums confidence will decrease proptionally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction:\n",
    "    This experiment isnt planning to use any mechine learning tools.although there are a pluria of tools for mechine learning out there such TensorFlow. TensorFlow was design and maintained by the Alphabet cooperation since novermeber,9th,2015. And is a good starting point for anyone who is planning to do production grade mechine learning aligrothiums for industry. However it is more partical and scientifical sound to build a mechine learning algrotium from fundumental calculus for the sake of controlling all the variables of the experiment. mechine learning began in the late 40's by computer scientist ....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method:\n",
    "    .....\n",
    "Addition:\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.36070544]\n",
      " [0.30259918]\n",
      " [0.14722428]]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Whole Class with additions:\n",
    "class Neural_Network(object):\n",
    "    def __init__(self):        \n",
    "        #Define Hyperparameters\n",
    "        self.inputLayerSize = 2\n",
    "        self.outputLayerSize = 1\n",
    "        self.hiddenLayerSize = 3\n",
    "        \n",
    "        #Weights (parameters)\n",
    "        self.W1 = np.random.randn(self.inputLayerSize,self.hiddenLayerSize)\n",
    "        self.W2 = np.random.randn(self.hiddenLayerSize,self.outputLayerSize)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        #Propogate inputs though network\n",
    "        self.z2 = np.dot(X, self.W1)\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W2)\n",
    "        yHat = self.sigmoid(self.z3) \n",
    "        return yHat\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        #Apply sigmoid activation function to scalar, vector, or matrix\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def sigmoidPrime(self,z):\n",
    "        #Gradient of sigmoid\n",
    "        return np.exp(-z)/((1+np.exp(-z))**2)\n",
    "    \n",
    "    def costFunction(self, X, y):\n",
    "        #Compute cost for given X,y, use weights already stored in class.\n",
    "        self.yHat = self.forward(X)\n",
    "        J = 0.5*sum((y-self.yHat)**2)\n",
    "        return J\n",
    "        \n",
    "    def costFunctionPrime(self, X, y):\n",
    "        #Compute derivative with respect to W and W2 for a given X and y:\n",
    "        self.yHat = self.forward(X)\n",
    "        \n",
    "        delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3))\n",
    "        dJdW2 = np.dot(self.a2.T, delta3)\n",
    "        \n",
    "        delta2 = np.dot(delta3, self.W2.T)*self.sigmoidPrime(self.z2)\n",
    "        dJdW1 = np.dot(X.T, delta2)  \n",
    "        \n",
    "        return dJdW1, dJdW2\n",
    "NN = Neural_Network()\n",
    "\n",
    "X = np.array(([3,5], [5,1], [10,2]), dtype=float)\n",
    "y = np.array(([75], [82], [93]), dtype=float)\n",
    "X = X/np.amax(X, axis=0)\n",
    "y = y/100 #Max test score is 100\n",
    "\n",
    "n = NeuralNetwork()\n",
    "print(n.forward(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
